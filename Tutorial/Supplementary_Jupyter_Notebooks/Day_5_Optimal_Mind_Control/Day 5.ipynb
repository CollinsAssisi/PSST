{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 5: Optimal Mind Control\n",
    "\n",
    "Welcome to Day 6! Now that we can simulate a model of a network of conductance-based neurons, we discuss the limitations of our algorithm and try to find solutions to the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management\n",
    "\n",
    "This TensorFlow implementation allows us to make our simulation code not only easier to read but also makes it highly parallizable and scalable across a variety of computational devices. But there are some major limitations to our system. The biggest of these issues is that despite making the simulation faster, this implementation makes it very memory intensive.\n",
    "\n",
    "The iterators in TensorFlow do not follow the normal process of Memory Allocation and Garbage Collection. Since, TensorFlow is designed to work on sophisticated hardware like GPUs, TPUs and distributed platforms, the memory allocation is done during TensorFlow session adaptively, but the memory is NOT cleared until the python kernel has stopped execution. \n",
    "\n",
    "The memory used increases linearly with time as the state matrix is computed recursively by the Scan function. The maximum memory used by the computational graph is 2 times the total state matrix size at the point when the computation finishes and copies the final data into the memory. Larger and more complicated the network and longer the simulation, larger the matrix and then, each run is limited by the total available memory. As we increase the number of neurons (n) and scale the degree distributions, the size of the memory used grows ~$O(n^2)$ as the number of differential equations grows as a square of the number of neurons. Similarly, as we increase the length of the simulation(L), the memory used grows $O(L)$. Which is why for a system with a limited memory of K bytes, The length of a given simulation (L timesteps) of a given network (N differential equations) with 64-bit floating-point precision will follow: \n",
    "\n",
    "$$2\\times64\\times L\\times N=K$$ \n",
    "\n",
    "That is, for any given network, our maximum simulation length is limited. One way to improve our maximum length is to divide the simulation into smaller batches. There will be a small queuing time between batches, which will slow down our code by a small amount but we will be able to simulate longer times. Thus, if we split the simulation into K sequential batches, the maximum memory for the simulation becomes $(1+\\frac{1}{K})$ times the total matrix size. Thus the memory relation becomes:  \n",
    "\n",
    "$$\\Big(1+\\frac{1}{K}\\Big)\\times64\\times L\\times N=K$$ \n",
    "\n",
    "This way, we can maximize the length of out simulation that we can run in a single python kernel.\n",
    "\n",
    "Let us implement this batch system for our 3 neuron feed-forward model.\n",
    "\n",
    "### Implementing the Model\n",
    "\n",
    "To improve out readability and make our code more modular we seperate the integrator into a independent import module. Take the integrator code that we developed in the last day and place it in a file called \"tf_integrator.py\". Make sure the file is present in the same directory as the implementation of the model. \n",
    "\n",
    "Note: If you are using Jupyter Notebook, remember to remove the %matplotlib inline command as it is specific to jupyter.\n",
    "\n",
    "#### Importing tf_integrator and other requirements\n",
    "\n",
    "Once the Integrator is saved in tf_integrator.py in the same directory as the Notebook, we can start importing the essentials including the integrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tf_integrator as tf_int\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall the Model\n",
    "\n",
    "For implementing a Batch system, we do not need to change how we construct our model only how we execute it.\n",
    "\n",
    "#### Step 1: Initialize Parameters and Dynamical Equations; Define Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_n = 3 # number of simultaneous neurons to simulate\n",
    "\n",
    "sim_res = 0.01                     # Time Resolution of the Simulation\n",
    "sim_time = 700                     # Length of the Simulation\n",
    "t = np.arange(0,sim_time,sim_res) \n",
    "\n",
    "# Acetylcholine\n",
    "\n",
    "ach_mat = np.zeros((n_n,n_n))        # Ach Synapse Connectivity Matrix\n",
    "ach_mat[1,0]=1\n",
    "\n",
    "## PARAMETERS FOR ACETYLCHLOLINE SYNAPSES ##\n",
    "\n",
    "n_ach = int(np.sum(ach_mat))     # Number of Acetylcholine (Ach) Synapses \n",
    "alp_ach = [10.0]*n_ach           # Alpha for Ach Synapse\n",
    "bet_ach = [0.2]*n_ach            # Beta for Ach Synapse\n",
    "t_max = 0.3                          # Maximum Time for Synapse\n",
    "t_delay = 0                          # Axonal Transmission Delay\n",
    "A = [0.5]*n_n                        # Synaptic Response Strength\n",
    "g_ach = [0.35]*n_n                   # Ach Conductance\n",
    "E_ach = [0.0]*n_n                    # Ach Potential\n",
    "\n",
    "# GABAa\n",
    "\n",
    "gaba_mat = np.zeros((n_n,n_n))       # GABAa Synapse Connectivity Matrix\n",
    "gaba_mat[2,1] = 1\n",
    "\n",
    "## PARAMETERS FOR GABAa SYNAPSES ##\n",
    "\n",
    "n_gaba = int(np.sum(gaba_mat)) # Number of GABAa Synapses\n",
    "alp_gaba = [10.0]*n_gaba       # Alpha for GABAa Synapse\n",
    "bet_gaba = [0.16]*n_gaba       # Beta for GABAa Synapse\n",
    "V0 = [-20.0]*n_n                     # Decay Potential\n",
    "sigma = [1.5]*n_n                    # Decay Time Constant\n",
    "g_gaba = [0.8]*n_n                  # fGABA Conductance\n",
    "E_gaba = [-70.0]*n_n                # fGABA Potential\n",
    "\n",
    "## Storing Firing Thresholds ##\n",
    "\n",
    "F_b = [0.0]*n_n                      # Fire threshold\n",
    "\n",
    "def I_inj_t(t):\n",
    "    return tf.constant(current_input.T,dtype=tf.float64)[tf.to_int32(t/sim_res)] # Turn indices to integer and extract from matrix\n",
    "\n",
    "## Acetylcholine Synaptic Current ##\n",
    "\n",
    "def I_ach(o,V):\n",
    "    o_ = tf.Variable([0.0]*n_n**2,dtype=tf.float64)\n",
    "    ind = tf.boolean_mask(tf.range(n_n**2),ach_mat.reshape(-1) == 1)\n",
    "    o_ = tf.scatter_update(o_,ind,o)\n",
    "    o_ = tf.transpose(tf.reshape(o_,(n_n,n_n)))\n",
    "    return tf.reduce_sum(tf.transpose((o_*(V-E_ach))*g_ach),1)\n",
    "\n",
    "## GABAa Synaptic Current ##\n",
    "\n",
    "def I_gaba(o,V):\n",
    "    o_ = tf.Variable([0.0]*n_n**2,dtype=tf.float64)\n",
    "    ind = tf.boolean_mask(tf.range(n_n**2),gaba_mat.reshape(-1) == 1)\n",
    "    o_ = tf.scatter_update(o_,ind,o)\n",
    "    o_ = tf.transpose(tf.reshape(o_,(n_n,n_n)))\n",
    "    return tf.reduce_sum(tf.transpose((o_*(V-E_gaba))*g_gaba),1)\n",
    "\n",
    "## Other Currents ##\n",
    "\n",
    "def I_K(V, n):\n",
    "    return g_K  * n**4 * (V - E_K)\n",
    "\n",
    "def I_Na(V, m, h):\n",
    "    return g_Na * m**3 * h * (V - E_Na)\n",
    "\n",
    "def I_L(V):\n",
    "    return g_L * (V - E_L)\n",
    "\n",
    "def dXdt(X, t):\n",
    "    V = X[:1*n_n]       # First n_n values are Membrane Voltage\n",
    "    m = X[1*n_n:2*n_n]  # Next n_n values are Sodium Activation Gating Variables\n",
    "    h = X[2*n_n:3*n_n]  # Next n_n values are Sodium Inactivation Gating Variables\n",
    "    n = X[3*n_n:4*n_n]  # Next n_n values are Potassium Gating Variables\n",
    "    o_ach = X[4*n_n : 4*n_n + n_ach] # Next n_ach values are Acetylcholine Synapse Open Fractions\n",
    "    o_gaba = X[4*n_n + n_ach : 4*n_n + n_ach + n_gaba] # Next n_ach values are GABAa Synapse Open Fractions\n",
    "    fire_t = X[-n_n:]   # Last n_n values are the last fire times as updated by the modified integrator\n",
    "    \n",
    "    dVdt = (I_inj_t(t) - I_Na(V, m, h) - I_K(V, n) - I_L(V) - I_ach(o_ach,V) - I_gaba(o_gaba,V)) / C_m \n",
    "    \n",
    "    ## Updation for gating variables ##\n",
    "    \n",
    "    m0,tm,h0,th = Na_prop(V)\n",
    "    n0,tn = K_prop(V)\n",
    "\n",
    "    dmdt = - (1.0/tm)*(m-m0)\n",
    "    dhdt = - (1.0/th)*(h-h0)\n",
    "    dndt = - (1.0/tn)*(n-n0)\n",
    "    \n",
    "    ## Updation for o_ach ##\n",
    "    \n",
    "    A_ = tf.constant(A,dtype=tf.float64)\n",
    "    Z_ = tf.zeros(tf.shape(A_),dtype=tf.float64)\n",
    "    \n",
    "    T_ach = tf.where(tf.logical_and(tf.greater(t,fire_t+t_delay),tf.less(t,fire_t+t_max+t_delay)),A_,Z_) \n",
    "    T_ach = tf.multiply(tf.constant(ach_mat,dtype=tf.float64),T_ach)\n",
    "    T_ach = tf.boolean_mask(tf.reshape(T_ach,(-1,)),ach_mat.reshape(-1) == 1)\n",
    "    \n",
    "    do_achdt = alp_ach*(1.0-o_ach)*T_ach - bet_ach*o_ach\n",
    "    \n",
    "    ## Updation for o_gaba ##\n",
    "        \n",
    "    T_gaba = 1.0/(1.0+tf.exp(-(V-V0)/sigma))\n",
    "    T_gaba = tf.multiply(tf.constant(gaba_mat,dtype=tf.float64),T_gaba)\n",
    "    T_gaba = tf.boolean_mask(tf.reshape(T_gaba,(-1,)),gaba_mat.reshape(-1) == 1)\n",
    "    \n",
    "    do_gabadt = alp_gaba*(1.0-o_gaba)*T_gaba - bet_gaba*o_gaba\n",
    "    \n",
    "    ## Updation for fire times ##\n",
    "    \n",
    "    dfdt = tf.zeros(tf.shape(fire_t),dtype=fire_t.dtype) # zero change in fire_t\n",
    "    \n",
    "\n",
    "    out = tf.concat([dVdt,dmdt,dhdt,dndt,do_achdt,do_gabadt,dfdt],0)\n",
    "    return out\n",
    "\n",
    "def K_prop(V):\n",
    "    T = 22\n",
    "    phi = 3.0**((T-36.0)/10)\n",
    "    V_ = V-(-50)\n",
    "    \n",
    "    alpha_n = 0.02*(15.0 - V_)/(tf.exp((15.0 - V_)/5.0) - 1.0)\n",
    "    beta_n = 0.5*tf.exp((10.0 - V_)/40.0)\n",
    "    \n",
    "    t_n = 1.0/((alpha_n+beta_n)*phi)\n",
    "    n_0 = alpha_n/(alpha_n+beta_n)\n",
    "    \n",
    "    return n_0, t_n\n",
    "\n",
    "\n",
    "def Na_prop(V):\n",
    "    T = 22\n",
    "    phi = 3.0**((T-36)/10)\n",
    "    V_ = V-(-50)\n",
    "    \n",
    "    alpha_m = 0.32*(13.0 - V_)/(tf.exp((13.0 - V_)/4.0) - 1.0)\n",
    "    beta_m = 0.28*(V_ - 40.0)/(tf.exp((V_ - 40.0)/5.0) - 1.0)\n",
    "    \n",
    "    alpha_h = 0.128*tf.exp((17.0 - V_)/18.0)\n",
    "    beta_h = 4.0/(tf.exp((40.0 - V_)/5.0) + 1.0)\n",
    "    \n",
    "    t_m = 1.0/((alpha_m+beta_m)*phi)\n",
    "    t_h = 1.0/((alpha_h+beta_h)*phi)\n",
    "    \n",
    "    m_0 = alpha_m/(alpha_m+beta_m)\n",
    "    h_0 = alpha_h/(alpha_h+beta_h)\n",
    "    \n",
    "    return m_0, t_m, h_0, t_h\n",
    "\n",
    "\n",
    "# Initializing the Parameters\n",
    "\n",
    "C_m = [1.0]*n_n\n",
    "g_K = [10.0]*n_n\n",
    "E_K = [-95.0]*n_n\n",
    "\n",
    "g_Na = [100]*n_n\n",
    "E_Na = [50]*n_n \n",
    "\n",
    "g_L = [0.15]*n_n\n",
    "E_L = [-55.0]*n_n\n",
    "\n",
    "# Creating the Current Input\n",
    "current_input= np.zeros((n_n,t.shape[0]))\n",
    "current_input[0,int(100/sim_res):int(200/sim_res)] = 2.5\n",
    "current_input[0,int(300/sim_res):int(400/sim_res)] = 5.0\n",
    "current_input[0,int(500/sim_res):int(600/sim_res)] = 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Define the Initial Condition of the Network and Add some Noise to the initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the State Vector and adding 1% noise\n",
    "state_vector = [-71]*n_n+[0,0,0]*n_n+[0]*n_ach+[0]*n_gaba+[-9999999]*n_n\n",
    "state_vector = np.array(state_vector)\n",
    "state_vector = state_vector + 0.01*state_vector*np.random.normal(size=state_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Splitting Time Series into independent batches and Run Each Batch Sequentially\n",
    "\n",
    "Since we will be dividing the computation into batches, we have to split the time array into batches which will be passed to the each successive call to the integrator. For each new call, the last state vector of the last call will be the new initial condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Number of Batches\n",
    "n_batch = 2\n",
    "\n",
    "# Split t array into batches using numpy\n",
    "t_batch = np.array_split(t,n_batch)\n",
    "\n",
    "# Iterate over the batches of time array\n",
    "for n,i in enumerate(t_batch):\n",
    "    \n",
    "    # Inform start of Batch Computation\n",
    "    print(\"Batch\",(n+1),\"Running...\",end=\"\")\n",
    "    \n",
    "    # In np.array_split(), the split edges are present in only one array and since \n",
    "    # our initial vector to successive calls is corresposnding to the last output\n",
    "    # our first element in the later time array should be the last element of the \n",
    "    # previous output series, Thus, we append the last time to the beginning of \n",
    "    # the current time array batch.\n",
    "    if n>0:\n",
    "        i = np.append(i[0]-sim_res,i)\n",
    "    \n",
    "    # Set state_vector as the initial condition\n",
    "    init_state = tf.constant(state_vector, dtype=tf.float64)\n",
    "    # Create the Integrator computation graph over the current batch of t array\n",
    "    tensor_state = tf_int.odeint(dXdt, init_state, i, n_n, F_b)\n",
    "    \n",
    "    # Initialize variables and run session\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        state = sess.run(tensor_state)\n",
    "        sess.close()\n",
    "    \n",
    "    # Reset state_vector as the last element of output\n",
    "    state_vector = state[-1,:]\n",
    "    \n",
    "    # Save the output of the simulation to a binary file\n",
    "    np.save(\"part_\"+str(n+1),state)\n",
    "\n",
    "    # Clear output\n",
    "    state=None\n",
    "    \n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting the Output Together\n",
    "\n",
    "The output from our batch implementation is a set of binary files that store subparts of our total simulation. To get the overall output we have to stitch them back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_state = []\n",
    "\n",
    "# Iterate over the generated output files\n",
    "for n,i in enumerate([\"part_\"+str(n+1)+\".npy\" for n in range(n_batch)]):\n",
    "    \n",
    "    # Since the first element in the series was the last output, we remove them\n",
    "    if n>0:\n",
    "        overall_state.append(np.load(i)[1:,:])\n",
    "    else:\n",
    "        overall_state.append(np.load(i))\n",
    "\n",
    "# Concatenate all the matrix to get a single state matrix\n",
    "overall_state = np.concatenate(overall_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Overall Data\n",
    "Finally, we plot the voltage traces of the 3 neurons as a Voltage vs Time heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "    \n",
    "sns.heatmap(overall_state[::100,:3].T,xticklabels=100,yticklabels=5,cmap='RdBu_r')\n",
    "\n",
    "plt.xlabel(\"Time (in ms)\")\n",
    "plt.ylabel(\"Neuron Number\")\n",
    "plt.title(\"Voltage vs Time Heatmap for Projection Neurons (PNs)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this method, we have maximized the usage of our available memory but we can go further and develop a method to allow indefinitely long simulation. The issue behind this entire algorithm is that the memory is not cleared until the python kernel finishes. One way to overcome this is to save the parameters of the model (such as connectivity matrix) and the state vector in a file, and start a new python kernel from a python script to compute successive batches. This way after each large batch, the memory gets cleaned. By combining the previous batch implementation and this system, we can maximize our computability.\n",
    "\n",
    "### Implementing a Runner and a Caller\n",
    "\n",
    "Firstly, we have to create an implementation of the model that takes in previous input as current parameters. Thus, we create a file, which we call \"run.py\" that takes an argument ie. the current batch number. The implementation for \"run.py\" is mostly same as the above model but there is a small difference.\n",
    "\n",
    "When the batch number is 0, we initialize all variable parameters and save them, but otherwise we use the saved values. The parameters we save include: Acetylcholine Matrix, GABAa Matrix and Final/Initial State Vector. It will also save the files with both batch number and sub-batch number listed.\n",
    "\n",
    "The time series will be created and split initially by the caller, which we call \"call.py\", and stored in a file. Each execution of the Runner will extract its relevant time series and compute on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Caller code\n",
    "\n",
    "The caller will create the time series, split it and use python subprocess module to call \"run.py\" with appropriate arguments. The code for \"call.py\" is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import call\n",
    "import numpy as np\n",
    "\n",
    "total_time = 700\n",
    "n_splits = 2\n",
    "time = np.split(np.arange(0,total_time,0.01),n_splits)\n",
    "\n",
    "# Append the last time point to the beginning of the next batch\n",
    "for n,i in enumerate(time):\n",
    "    if n>0:\n",
    "        time[n] = np.append(i[0]-0.01,i)\n",
    "\n",
    "np.save(\"time\",time)\n",
    "\n",
    "# call successive batches with a new python subprocess and pass the batch number\n",
    "for i in range(n_splits):\n",
    "    call(['python','run.py',str(i)])\n",
    "\n",
    "print(\"Simulation Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Runner code\n",
    "\n",
    "\"run.py\" is essentially identical to the batch-implemented model we developed above with the changes described below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Imports #\n",
    "\n",
    "import sys\n",
    "\n",
    "# Duration of Simulation #\n",
    "\n",
    "# t = np.arange(0,sim_time,sim_res)      \n",
    "t = np.load(\"time.npy\")[int(sys.argv[1])] # get first argument to run.py\n",
    "\n",
    "# Connectivity Matrix Definitions #\n",
    "\n",
    "if sys.argv[1] == '0':\n",
    "    ach_mat = np.zeros((n_n,n_n)) # Ach Synapse Connectivity Matrix\n",
    "    ach_mat[1,0]=1                # If connectivity is random, once initialized it will be the same.\n",
    "    np.save(\"ach_mat\",ach_mat)\n",
    "else:\n",
    "    ach_mat = np.load(\"ach_mat.npy\")\n",
    "    \n",
    "if sys.argv[1] == '0':\n",
    "    gaba_mat = np.zeros((n_n,n_n)) # GABAa Synapse Connectivity Matrix\n",
    "    gaba_mat[2,1] = 1              # If connectivity is random, once initialized it will be the same.\n",
    "    np.save(\"gaba_mat\",gaba_mat)\n",
    "else:\n",
    "    gaba_mat = np.load(\"gaba_mat.npy\")\n",
    "\n",
    "# Current Input Definition #\n",
    "    \n",
    "if sys.argv[1] == '0':\n",
    "    current_input= np.zeros((n_n,int(sim_time/sim_res)))\n",
    "    current_input[0,int(100/sim_res):int(200/sim_res)] = 2.5\n",
    "    current_input[0,int(300/sim_res):int(400/sim_res)] = 5.0\n",
    "    current_input[0,int(500/sim_res):int(600/sim_res)] = 7.5\n",
    "    np.save(\"current_input\",current_input)\n",
    "else:\n",
    "    current_input = np.load(\"current_input.npy\")\n",
    "    \n",
    "# State Vector Definition #\n",
    "\n",
    "if sys.argv[1] == '0':\n",
    "    sstate_vector = [-71]*n_n+[0,0,0]*n_n+[0]*n_ach+[0]*n_gaba+[-9999999]*n_n\n",
    "    state_vector = np.array(state_vector)\n",
    "    state_vector = state_vector + 0.01*state_vector*np.random.normal(size=state_vector.shape)\n",
    "    np.save(\"state_vector\",state_vector)\n",
    "else:\n",
    "    state_vector = np.load(\"state_vector.npy\")\n",
    "\n",
    "# Saving of Output #\n",
    "\n",
    "# np.save(\"part_\"+str(n+1),state)\n",
    "np.save(\"batch\"+str(int(sys.argv[1])+1)+\"_part_\"+str(n+1),state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining all Data\n",
    "\n",
    "Just like we merged all the batches, we merge all the sub-batches and batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_state = []\n",
    "\n",
    "# Iterate over the generated output files\n",
    "for n,i in enumerate([\"batch\"+str(x+1) for x in range(n_splits)]):\n",
    "    for m,j in enumerate([\"_part_\"+str(x+1)+\".npy\" for x in range(n_batch)]):\n",
    "    \n",
    "        # Since the first element in the series was the last output, we remove them\n",
    "        if n>0 and m>0:\n",
    "            overall_state.append(np.load(i+j)[1:,:])\n",
    "        else:\n",
    "            overall_state.append(np.load(i+j))\n",
    "\n",
    "# Concatenate all the matrix to get a single state matrix\n",
    "overall_state = np.concatenate(overall_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "    \n",
    "sns.heatmap(overall_state[::100,:3].T,xticklabels=100,yticklabels=5,cmap='RdBu_r')\n",
    "\n",
    "plt.xlabel(\"Time (in ms)\")\n",
    "plt.ylabel(\"Neuron Number\")\n",
    "plt.title(\"Voltage vs Time Heatmap for Projection Neurons (PNs)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Limitations\n",
    "\n",
    "With the runner-caller implementation, we have solved the issue of memory limitation. But there are some issues we havent solved yet.\n",
    "\n",
    "1. **Delay Dependent Activity:** If the implementation of an model requires a delay differential equation (DDEs) that accesses the value of the parameters at a earlier time, we cannot use this programming paradigm directly as it has only, at most, one timestep memory. This is also a limitation of using the scan function. Exploring alternatives to tf.scan() and other techniques of solving DDEs is an option.\n",
    "\n",
    "2. **Optimal Distributed Computing:** To maximize the utility of distributed systems, the algorithm needs to be capable of splitting the computation into tasks that can be run parallely (simultaneously) in different computational units. Since we are working with numerical integration, the results of the last step need to be known to perform the next computation. This, in a way limits out ability to use distributed computing. TensorFlow automatically tries to utilize all available resources efficiently but it may not be the optimal usage, for example, by default, it cannot utilize more than one GPU or access other nodes in a cluster. One way to enforce distribution is to manually distribute parts of the computational graph to specialized computing units. For example, the actual session can be forced to be executed on the server with maximum memory but the sub-computations such as the RK4 integration steps can be distributed to servers with more GPUs. See Supplementary on Distributed Computing for more Information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
